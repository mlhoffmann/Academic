{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential, load_model, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import MaxPooling1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from rfpimp import permutation_importances\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-52468c3d329f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data.xlsx'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m# df.head()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m                 )\n\u001B[0;32m    295\u001B[0m                 \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mFutureWarning\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 304\u001B[1;33m         \u001B[0mio\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    305\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m         raise ValueError(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, path_or_buffer, engine)\u001B[0m\n\u001B[0;32m    865\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_io\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstringify_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    866\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 867\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engines\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_io\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    868\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    869\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__fspath__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filepath_or_buffer)\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0merr_msg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mimport_optional_dependency\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"xlrd\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mextra\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merr_msg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filepath_or_buffer)\u001B[0m\n\u001B[0;32m    351\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbook\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_workbook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 353\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbook\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_workbook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    354\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbytes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    355\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbook\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_workbook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBytesIO\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001B[0m in \u001B[0;36mload_workbook\u001B[1;34m(self, filepath_or_buffer)\u001B[0m\n\u001B[0;32m     35\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mopen_workbook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_contents\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mopen_workbook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001B[0m in \u001B[0;36mopen_workbook\u001B[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpanduser\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m             \u001B[0mpeek\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpeeksz\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mpeek\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34mb\"PK\\x03\\x04\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;31m# a ZIP file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data.xlsx'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Data.xlsx')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df['Data'], format='%Y.%m.%d.%H.%M.%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Data',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando os dados de treino e de teste:\n",
    "========================================\n",
    "Este período foi selecionado devido ser onde o compressor operou de forma constante sem falhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df['2018-08-02 01:00:00':'2018-09-20 00:00:00'] \n",
    "test =  df['2018-09-20 00:00:00':'2018-10-01 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normalizar dados:**\n",
    "\n",
    "Em seguida, uso as ferramentas de pré-processamento do Scikit-learn para dimensionar as variáveis de entrada do modelo. \n",
    "O “MinMaxScaler” simplesmente redimensiona os dados para estar no intervalo [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "scaler_filename = \"scaler_data\"\n",
    "dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão para float32 \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshape inputs for LSTM [samples, timesteps, features]\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "print(\"Training data shape:\", X_train_cnn.shape)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "print(\"Test data shape:\", X_test_cnn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar para teste\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(train), \n",
    "                              columns=train.columns, \n",
    "                              index=train.index)\n",
    "# Utilizado Random shuffle nos dados de treinamento para selecionar de forma aleatória\n",
    "X_train.sample(frac=1)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(test), \n",
    "                             columns=test.columns, \n",
    "                             index=test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros do modelo\n",
    "\n",
    "# Dimensões input\n",
    "INPUT_SHAPE = (X_train.shape[1],1) \n",
    "\n",
    "# Tamanho do batch\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Dropout\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Número de épocas\n",
    "EPOCHS = 25\n",
    "\n",
    "# Dados para Split\n",
    "SPLIT_VAL=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arquitetura do Modelo CNN\n",
    "  \n",
    "model = Sequential([\n",
    "        \n",
    "# Primeira camada convolucional com duas operações seguidas de convolução   \n",
    "Conv1D(128, kernel_size = 7, activation = 'relu', padding = 'same', input_shape = INPUT_SHAPE),\n",
    "MaxPooling1D(pool_size = 2),      \n",
    "Dropout(DROPOUT_RATE), \n",
    "\n",
    "# Segunda camada convolucional com duas operações seguidas de convolução\n",
    "Conv1D(64, kernel_size = 5, activation = 'relu',padding = 'same'),\n",
    "MaxPooling1D(pool_size = 2),      \n",
    "Dropout(DROPOUT_RATE),\n",
    "\n",
    "    \n",
    "# Terceira camada convolucional com duas operações seguidas de convolução\n",
    "Conv1D(32, kernel_size = 3, activation = 'relu',padding = 'same'),     \n",
    "Conv1D(32, kernel_size = 3, activation = 'relu',padding = 'same'),     \n",
    "MaxPooling1D(pool_size = 2), \n",
    "Dropout(DROPOUT_RATE), \n",
    "    \n",
    "\n",
    "# Quinta camada convolucional com duas operações seguidas de convolução\n",
    "Conv1D(64, kernel_size = 5, activation = 'relu',padding = 'same'),\n",
    "MaxPooling1D(pool_size = 2), \n",
    "Dropout(DROPOUT_RATE), \n",
    "\n",
    "# Quinta camada convolucional com duas operações seguidas de convolução\n",
    "Conv1D(128, kernel_size = 7, activation = 'relu',padding = 'same'),\n",
    "Dropout(DROPOUT_RATE), \n",
    "    \n",
    "Flatten(),\n",
    "# Terceira camada totalmente conectada    \n",
    "Dense(100,activation = 'relu'),\n",
    "Dropout(DROPOUT_RATE), \n",
    "Dense(X_train.shape[1],activation = 'relu')\n",
    "])\n",
    "        \n",
    "# Otimizador Adam\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "#Print Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checando a memória da GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar recuso da memória da GPU\n",
    "pid = 6232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.kill(pid,pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=7, verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit(X_train_cnn, X_train, epochs=EPOCHS, callbacks= [monitor] , batch_size=BATCH_SIZE,\n",
    "                    validation_split=SPLIT_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.plot(history.history['val_loss'],\n",
    "         'r',\n",
    "         label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "plt.ylim([0,.08])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_train_cnn))\n",
    "X_pred = pd.DataFrame(X_pred, columns=X_train.columns)\n",
    "X_pred.index = X_train.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_train.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "plt.figure()\n",
    "sns.distplot(scored['Loss_mae'],\n",
    "             bins = 5, \n",
    "             kde= True,\n",
    "            color = 'blue');\n",
    "plt.xlim([0.0,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_test_cnn))\n",
    "X_pred = pd.DataFrame(X_pred, columns=X_test.columns)\n",
    "X_pred.index = X_test.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_test.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "scored['Threshold'] = 0.15\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_train = model.predict(np.array(X_train_cnn))\n",
    "X_pred_train = pd.DataFrame(X_pred_train, columns=X_train.columns)\n",
    "X_pred_train.index = X_train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=X_train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "scored_train['Threshold'] = 0.15\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored.plot(logy=True,  figsize = (10,6), ylim = [1e-3,1e2], color = ['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenado os dados de treino e teste para comparar com a função gerada pelo modelo\n",
    "df_TraTes = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando o Subset do resultado \n",
    "df0 = pd.DataFrame(scored.Loss_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando o resultado dos Scores\n",
    "df_fim = pd.merge(df_TraTes,df0, on=[\"Data\"])\n",
    "df_fim = df_fim.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a correlação com a perda \n",
    "df_corr = pd.DataFrame(df_fim.corr(method=\"pearson\")[\"Loss_mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_corr.dropna() # Remover coluna NaN Gerada na correlação\n",
    "df_corr.index.name = \"Tag's\" # Renomear o Index com os nomes dos Tag's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear coluna \n",
    "df_corr = df_corr.rename(columns={ df_corr.columns[0]: \"Correlation\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a Filtro para verificar correlação\n",
    "sel1 = df_corr.query('(Correlation > 0.3) | (Correlation < -0.4)').sort_values(by='Correlation', ascending=False)\n",
    "sel1.drop('Loss_mae',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">FEATURE IMPORTANCE</span>\n",
    "============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_fim.iloc[:,-1]\n",
    "X = df_fim.iloc[:,0:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisations\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc = {'figure.figsize':(30, 25)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating a feature importance dataframe\n",
    "def imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title):\n",
    "    imp_df.columns = ['feature', 'feature_importance']\n",
    "    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n",
    "       .set_title(title, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.65, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(rf.score(X_train, y_train), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imp = imp_df(X_train.columns, rf.feature_importances_)\n",
    "base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [13, 10]\n",
    "var_imp_plot(base_imp, \"Feature importance by - Tag's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(rf, X_train, y_train):\n",
    "    return r2_score(y_train, rf.predict(X_train))\n",
    "\n",
    "perm_imp_rfpimp = permutation_importances(rf, X_train, y_train, r2)\n",
    "perm_imp_rfpimp.reset_index(drop = False, inplace = True) #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_plot(perm_imp_rfpimp, 'Permutation feature importance (rfpimp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(rf, cv = None, refit = False, n_iter = 100).fit(X_train, y_train)\n",
    "perm_imp_eli5 = imp_df(X_train.columns, perm.feature_importances_)\n",
    "var_imp_plot(perm_imp_eli5, 'Permutation feature importance Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm,show_feature_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=150,\n",
    "                           booster='gbtree',\n",
    "                           importance_type='gain',\n",
    "                           gpu_id=-1,\n",
    "                           verbosity=1,\n",
    "                           random_state = 40)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('R^2 Training Score: {:.2f}  \\nR^2 Validation Score: {:.2f}'.format(xgb.score(X_train, y_train), \n",
    "                                                                          xgb.score(X_valid, y_valid)))                        \n",
    "                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imp_xgb = imp_df(X_train.columns, xgb.feature_importances_)\n",
    "base_imp_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [13, 10]\n",
    "var_imp_plot(base_imp_xgb, \"Feature importance by - Tag's - xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_xgb = PermutationImportance(xgb,  cv = None, refit = False, n_iter = 150).fit(X_train, y_train)\n",
    "perm_imp_eli5_xgb = imp_df(X_train.columns, perm_xgb.feature_importances_)\n",
    "var_imp_plot(perm_imp_eli5_xgb, 'Permutation feature importance xgboost ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_xgb,show_feature_values=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(rf, X_train, y_train, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_train.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a7e5b514",
   "language": "python",
   "display_name": "PyCharm (CNN Feature Selection)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}